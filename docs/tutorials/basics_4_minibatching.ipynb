{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load from parent directory if not installed\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "if not importlib.util.find_spec(\"sammo\"):\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Minibatching\n",
    "\n",
    "When annotating data, it is quite wasteful to use one LLM request per example, especially when the instructions are shared. \n",
    "\n",
    "Instead, we can use in-context minibatching to improve *call efficiency* to annotate multiple data examples in one call. Let's repeat the setup from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 3:25 _init.py\n",
    "import pathlib\n",
    "import sammo\n",
    "from sammo.runners import OpenAIChat\n",
    "from sammo.base import Template, EvaluationScore\n",
    "from sammo.components import Output, GenerateText, ForEach, Union\n",
    "from sammo.extractors import ExtractRegex\n",
    "from sammo.data import DataTable\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "if not \"OPENAI_API_KEY\" in os.environ:\n",
    "    raise ValueError(\"Please set the environment variable OPENAI_API_KEY'.\")\n",
    "\n",
    "_ = sammo.setup_logger(\"WARNING\")  # we're only interested in warnings for now\n",
    "\n",
    "runner = OpenAIChat(\n",
    "    model_id=\"gpt-3.5-turbo-16k\",\n",
    "    api_config={\"api_key\": os.getenv(\"OPENAI_API_KEY\")},\n",
    "    cache=os.getenv(\"CACHE_FILE\", \"cache.tsv\"),\n",
    "    timeout=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -s load_data,accuracy _init.py\n",
    "def load_data(\n",
    "    url=\"https://github.com/google/BIG-bench/raw/main/bigbench/benchmark_tasks/implicatures/task.json\",\n",
    "):\n",
    "    task = json.loads(requests.get(url).content)\n",
    "    # convert label to single string\n",
    "    for x in task[\"examples\"]:\n",
    "        x[\"output\"] = max(x[\"target_scores\"], key=x[\"target_scores\"].get)\n",
    "\n",
    "    return DataTable.from_records(\n",
    "        task[\"examples\"],\n",
    "        input_fields=\"input\",\n",
    "        constants={\"instructions\": task[\"task_prefix\"]},\n",
    "    )\n",
    "\n",
    "def accuracy(y_true: DataTable, y_pred: DataTable) -> EvaluationScore:\n",
    "    y_true = y_true.outputs.values\n",
    "    y_pred = y_pred.outputs.values\n",
    "    n_correct = sum([y_p == y_t for y_p, y_t in zip(y_pred, y_true)])\n",
    "\n",
    "    return EvaluationScore(n_correct / len(y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Manual Minibatching\n",
    "Let's start by doing manual minibatching. SAMMO will split inputs into minibatches of a specified size for us. \n",
    "The only thing we have to do is loop over the template variable `{{inputs}}` using the [handlebars syntax](https://handlebarsjs.com/guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeling_prompt = GenerateText(\n",
    "    Template(\n",
    "        \"Instructions:{{constants.instructions}}\\nOutput labels: yes, no\\n\"\n",
    "        \"{{#each inputs}}Input: {{this}}{{/each}}\\nOutput:\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The only other changes we need to make is to specify the minibatch size in the Output component and also make sure the output gets split into lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatches[##################################################################################]1/1[00:00<00:00, 62.50it/s]\n",
      "\n",
      "Exception: Minibatch results do not have right length (need: 10, got: 1)\n"
     ]
    }
   ],
   "source": [
    "labeling_outputter = Output(labeling_prompt, minibatch_size=10)\n",
    "mydata = load_data()\n",
    "sample = mydata.sample(10, seed=42)\n",
    "\n",
    "try:\n",
    "    result = labeling_outputter.run(runner, sample)\n",
    "except Exception as e:\n",
    "    print(f\"\\nException: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Oh, no - there is something wrong with the minibatch results. The number of answers we get from a single LLM call need to be aligned with all the input rows which is where we fail.\n",
    "\n",
    "Going back to the prompt, we realize that we forgot to extract all valid answers from the `GenerateText` call! Let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatches[##################################################################################]1/1[00:00<00:00, 32.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+--------------------------------------------------------------+----------+\n",
       "| input                                                        | output   |\n",
       "+==============================================================+==========+\n",
       "| Speaker 1: 'You do this often?' Speaker 2: 'It's my first    | yes      |\n",
       "| time.'                                                       |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you trying to make me mad?' Speaker 2: 'I'm  | no       |\n",
       "| just saying, I'd understand if you were upset. '             |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'You want answers?!' Speaker 2: 'I want the       | no       |\n",
       "| truth.'                                                      |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you able to carry the box?' Speaker 2: 'It   | yes      |\n",
       "| is as light as a feather.'                                   |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Is it hot outside?' Speaker 2: 'You could fry an | no       |\n",
       "| egg on the sidewalk.'                                        |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Should we repay you?' Speaker 2: 'There is no    | no       |\n",
       "| charge for awesomeness, or attractiveness.'                  |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'I wonder, Bob, if you can handle my car?'        | no       |\n",
       "| Speaker 2: 'It's an ordinary six cylinder.'                  |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Did you order the code red?' Speaker 2: 'You're  | yes      |\n",
       "| goddamn right.'                                              |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'You've seen rain before... right?' Speaker 2:    | no       |\n",
       "| 'We don't get out much.'                                     |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Does anyone know how to pick a lock?' Speaker 2: | yes      |\n",
       "| 'Sure. Picking locks is my thing.'                           |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "Constants: {'instructions': \"Does Speaker 2's answer mean yes or no? \"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling_outputter = Output(\n",
    "    ExtractRegex(labeling_prompt, \"(?i)yes|no\"), minibatch_size=10\n",
    ")\n",
    "result = labeling_outputter.run(runner, sample)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Nice! With a single LLM call, we now annotated 10 examples! It was, however, a bit annoying to have to manually format the minibatches. Luckily, `SAMMO` provides a `MetaTemplate` class for common data annotation tasks that simplifies the set-up considerably.\n",
    "\n",
    "## Automatic minibatching with the `MetaPrompt` component\n",
    "\n",
    "The `MetaPrompt` component takes a nested list of instructions, an argument specifying how instructions are rendered and a `DataFormatter` instance that is shared for in-context examples and input examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sammo.instructions import MetaPrompt, Section, Paragraph, InputData, FewshotExamples\n",
    "from sammo.dataformatters import (\n",
    "    QuestionAnswerFormatter,\n",
    "    JSONDataFormatter\n",
    ")\n",
    "\n",
    "mprompt = MetaPrompt(\n",
    "    [\n",
    "        Section(\"Instructions\", mydata.constants[\"instructions\"]),\n",
    "        Section(\"Examples\", FewshotExamples(mydata.sample(3, seed=43))),\n",
    "        Paragraph(\"\\nOutput labels: yes, no\"),\n",
    "        Paragraph(InputData()),\n",
    "    ],\n",
    "    render_as=\"markdown\",\n",
    "    data_formatter=QuestionAnswerFormatter([\"yes\", \"no\"]),\n",
    ")\n",
    "# automatically wraps it with the right parser component\n",
    "mprompt_parsed = mprompt.with_extractor(\"empty_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We have now structured our labeling task into a section and a few paragraphs. The `DataFormatter` class does all the data formatting for us, and calling `with_extractor()` wraps the response with the right extractor class to match our data formatter. We have also added a section with fewshot (incontext) examples to show the model how the output format looks.\n",
    "\n",
    "We can just look at the current metaprompt to see what was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "            <iframe srcdoc=\"\n",
       "&lt;!DOCTYPE html&gt;\n",
       "&lt;html lang=&quot;en&quot;&gt;\n",
       "\n",
       "&lt;head&gt;\n",
       "  &lt;style&gt;\n",
       "  body,\n",
       "  html {\n",
       "    margin: 0;\n",
       "    height: 100%;\n",
       "    width: 100%;\n",
       "  }\n",
       "\n",
       "  .split {\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    height: 100%;\n",
       "    overflow: hidden;\n",
       "  }\n",
       "  .gutter {\n",
       "    background-color: #eee;\n",
       "    background-repeat: no-repeat;\n",
       "    background-position: 50%;\n",
       "  }\n",
       "\n",
       "  .gutter.gutter-horizontal {\n",
       "    background-image: url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAeCAYAAADkftS9AAAAIklEQVQoU2M4c+bMfxAGAgYYmwGrIIiDjrELjpo5aiZeMwF+yNnOs5KSvgAAAABJRU5ErkJggg==&#x27;);\n",
       "    cursor: col-resize;\n",
       "  }\n",
       "  #info {\n",
       "\toverflow-y: auto;\n",
       "  }\n",
       "  #info div {\n",
       "    font-family: Helvetica, sans-serif;\n",
       "    font-weight: 600;\n",
       "    text-transform: uppercase;\n",
       "    background-color: #d6d8d9;\n",
       "    color: #616161;\n",
       "    font-size: 11px;\n",
       "    padding: 6px;\n",
       "  }\n",
       "  pre {\n",
       "\twhite-space: pre-wrap;\n",
       "\tpadding: 6px;\n",
       "\tbackground-color: #f9f9f9;\n",
       "\tmargin: 0;\n",
       "\tfont-size: 12px;\n",
       "  }\n",
       "  &lt;/style&gt;\n",
       "  &lt;meta charset=&quot;utf-8&quot; /&gt;\n",
       "  &lt;title&gt;Callgraph&lt;/title&gt;\n",
       "  &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/cytoscape/3.26.0/cytoscape.min.js&quot;&gt;&lt;/script&gt;\n",
       "  &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/split.js/1.6.0/split.min.js&quot;&gt;&lt;/script&gt;\n",
       "&lt;/head&gt;\n",
       "\n",
       "&lt;body&gt;\n",
       "  &lt;div class=&quot;split&quot;&gt;\n",
       "    &lt;div id=&quot;info&quot;&gt;&lt;pre&gt;Click on node for details.&lt;/pre&gt;&lt;/div&gt;\n",
       "    &lt;div id=&quot;graph&quot;&gt;&lt;/div&gt;\n",
       "  &lt;/div&gt;\n",
       "  &lt;script&gt;\n",
       "  Split([&#x27;#info&#x27;, &#x27;#graph&#x27;], {\n",
       "    sizes: [20, 80],\n",
       "    minSize: [150, 0],\n",
       "    onDragEnd: resizeCyto\n",
       "  });\n",
       "  const DATA = {&quot;nodes&quot;: [{&quot;data&quot;: {&quot;id&quot;: 1902863761168, &quot;label&quot;: &quot;ExtractRegex&quot;, &quot;priority&quot;: 0, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- child: GenerateText\\n- regex: ^\\\\s*A[^:]*:\\\\s*([^\\\\n]*)\\n- max_matches: None\\n- strip_whitespaces: True&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1903521046864, &quot;label&quot;: &quot;GenerateText&quot;, &quot;priority&quot;: 1, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- child: MetaPrompt\\n- reference_id: None\\n- system_prompt: None\\n- history: None\\n- seed: 0\\n- randomness: 0\\n- max_tokens: None\\n- json_mode: False\\n- on_error: empty_result\\n- runner: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902863393680, &quot;label&quot;: &quot;MetaPrompt&quot;, &quot;priority&quot;: 2, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- child: [\\n  0 : Section(\\n    title = &#x27;Instructions&#x27;,\\n    content = \\&quot;Does Speaker 2&#x27;s answer mean yes or no? \\&quot;,\\n    reference_id = None,\\n    reference_classes = None\\n  ),\\n  1 : Section(\\n    title = &#x27;Examples&#x27;,\\n    content = FewshotExamples(\\n      data = +--------------------------------------------------------------+----------+\\n| input                                                        | output   |\\n+==============================================================+==========+\\n| Speaker 1: &#x27;Should I bring my umbrella?&#x27; Speaker 2: &#x27;Better  | yes      |\\n| safe than sorry.&#x27;                                            |          |\\n+--------------------------------------------------------------+----------+\\n| Speaker 1: &#x27;Do you have a girl worth fighting for?&#x27; Speaker  | no       |\\n| 2: &#x27;Wish that I had.&#x27;                                        |          |\\n+--------------------------------------------------------------+----------+\\n| Speaker 1: &#x27;Do you think I should attend the interview?&#x27;     | yes      |\\n| Speaker 2: &#x27;Do you want to be a failure for the rest of your |          |\\n| life?&#x27;                                                       |          |\\n+--------------------------------------------------------------+----------+\\nConstants: {&#x27;instructions&#x27;: \\&quot;Does Speaker 2&#x27;s answer mean yes or no? \\&quot;},\\n      n_examples = None,\\n      reference_id = None\\n    ),\\n    reference_id = None,\\n    reference_classes = None\\n  ),\\n  2 : Paragraph(\\n    content = &#x27;\\\\nOutput labels: yes, no&#x27;,\\n    reference_id = None,\\n    reference_classes = None\\n  ),\\n  3 : Paragraph(\\n    content = InputData(\\n      id_offset = 0,\\n      reference_id = None\\n    ),\\n    reference_id = None,\\n    reference_classes = None\\n  )\\n]\\n- render_as: markdown\\n- data_formatter: QuestionAnswerFormatter\\n- reference_id: None\\n- seed: 0&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902867053776, &quot;label&quot;: &quot;Section&quot;, &quot;priority&quot;: 3, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- title: Instructions\\n- content: Does Speaker 2&#x27;s answer mean yes or no? \\n- reference_id: None\\n- reference_classes: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902862121808, &quot;label&quot;: &quot;Section&quot;, &quot;priority&quot;: 4, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- title: Examples\\n- content: FewshotExamples\\n- reference_id: None\\n- reference_classes: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902866858064, &quot;label&quot;: &quot;Paragraph&quot;, &quot;priority&quot;: 5, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- content: \\nOutput labels: yes, no\\n- reference_id: None\\n- reference_classes: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902845035792, &quot;label&quot;: &quot;Paragraph&quot;, &quot;priority&quot;: 6, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- content: InputData\\n- reference_id: None\\n- reference_classes: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902863677328, &quot;label&quot;: &quot;QuestionAnswerFormatter&quot;, &quot;priority&quot;: 7, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- all_labels: [\\n  0 : &#x27;yes&#x27;,\\n  1 : &#x27;no&#x27;\\n]&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902863712016, &quot;label&quot;: &quot;FewshotExamples&quot;, &quot;priority&quot;: 8, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- data: +--------------------------------------------------------------+----------+\\n| input                                                        | output   |\\n+==============================================================+==========+\\n| Speaker 1: &#x27;Should I bring my umbrella?&#x27; Speaker 2: &#x27;Better  | yes      |\\n| safe than sorry.&#x27;                                            |          |\\n+--------------------------------------------------------------+----------+\\n| Speaker 1: &#x27;Do you have a girl worth fighting for?&#x27; Speaker  | no       |\\n| 2: &#x27;Wish that I had.&#x27;                                        |          |\\n+--------------------------------------------------------------+----------+\\n| Speaker 1: &#x27;Do you think I should attend the interview?&#x27;     | yes      |\\n| Speaker 2: &#x27;Do you want to be a failure for the rest of your |          |\\n| life?&#x27;                                                       |          |\\n+--------------------------------------------------------------+----------+\\nConstants: {&#x27;instructions&#x27;: \\&quot;Does Speaker 2&#x27;s answer mean yes or no? \\&quot;}\\n- n_examples: None\\n- reference_id: None&quot;}}}, {&quot;data&quot;: {&quot;id&quot;: 1902864221392, &quot;label&quot;: &quot;InputData&quot;, &quot;priority&quot;: 9, &quot;details&quot;: {&quot;Parameters&quot;: &quot;- id_offset: 0\\n- reference_id: None&quot;}}}], &quot;edges&quot;: [{&quot;data&quot;: {&quot;target&quot;: 1902863761168, &quot;source&quot;: 1903521046864}}, {&quot;data&quot;: {&quot;target&quot;: 1903521046864, &quot;source&quot;: 1902863393680}}, {&quot;data&quot;: {&quot;target&quot;: 1902863393680, &quot;source&quot;: 1902867053776}}, {&quot;data&quot;: {&quot;target&quot;: 1902863393680, &quot;source&quot;: 1902862121808}}, {&quot;data&quot;: {&quot;target&quot;: 1902863393680, &quot;source&quot;: 1902866858064}}, {&quot;data&quot;: {&quot;target&quot;: 1902863393680, &quot;source&quot;: 1902845035792}}, {&quot;data&quot;: {&quot;target&quot;: 1902863393680, &quot;source&quot;: 1902863677328}}, {&quot;data&quot;: {&quot;target&quot;: 1902862121808, &quot;source&quot;: 1902863712016}}, {&quot;data&quot;: {&quot;target&quot;: 1902845035792, &quot;source&quot;: 1902864221392}}], &quot;node-color&quot;: &quot;white&quot;, &quot;node-border&quot;: 1};\n",
       "\n",
       "  var cy = cytoscape({\n",
       "    container: document.getElementById(&#x27;graph&#x27;),\n",
       "    style: cytoscape.stylesheet().selector(&#x27;node&#x27;).style({\n",
       "      &#x27;content&#x27;: &#x27;data(label)&#x27;,\n",
       "\t  &#x27;background-color&#x27;: DATA[&#x27;node-color&#x27;] || &#x27;Teal&#x27;,\n",
       "\t  &#x27;border-width&#x27;: DATA[&#x27;node-border&#x27;] || 0,\n",
       "    }).selector(&#x27;:selected&#x27;).style({&#x27;background-color&#x27;: &#x27;Turquoise&#x27;}).selector(&#x27;edge&#x27;).style({\n",
       "      &#x27;curve-style&#x27;: &#x27;bezier&#x27;,\n",
       "      &#x27;target-arrow-shape&#x27;: &#x27;triangle&#x27;,\n",
       "      &#x27;width&#x27;: 1,\n",
       "      &#x27;line-color&#x27;: &#x27;black&#x27;,\n",
       "      &#x27;target-arrow-color&#x27;: &#x27;black&#x27;\n",
       "    }),\n",
       "    elements: DATA,\n",
       "    wheelSensitivity: 0.3,\n",
       "    layout: {\n",
       "      name: &#x27;breadthfirst&#x27;,\n",
       "      directed: true,\n",
       "      depthSort: function(a, b){ return a.data(&#x27;priority&#x27;) - b.data(&#x27;priority&#x27;) }\n",
       "    }\n",
       "  });\n",
       "\n",
       "  function resizeCyto() {\n",
       "    cy.resize();\n",
       "    cy.fit();\n",
       "  }\n",
       "\n",
       "  function escapeHTML(str){\n",
       "    return new Option(str).innerHTML;\n",
       "  }\n",
       "\n",
       "  window.addEventListener(&#x27;resize&#x27;, resizeCyto);\n",
       "  cy.on(&#x27;tap&#x27;, &#x27;node&#x27;, function(evt) {\n",
       "    const node = evt.target;\n",
       "    const details = node.data(&#x27;details&#x27;);\n",
       "    const info = document.getElementById(&#x27;info&#x27;);\n",
       "    info.innerHTML = &#x27;&#x27;;\n",
       "\n",
       "    if (!details) {\n",
       "        info.innerHTML = &quot;&lt;div&gt;Node has no metadata.&lt;/div&gt;&quot;;\n",
       "    } else if (typeof details === &#x27;object&#x27; &amp;&amp; !Array.isArray(details)) {\n",
       "        Object.entries(details).forEach(([key, value]) =&gt; {\n",
       "            info.innerHTML += `&lt;div&gt;${escapeHTML(key)}&lt;/div&gt;&lt;pre&gt;${escapeHTML(value)}&lt;/pre&gt;`;\n",
       "        });\n",
       "    } else {\n",
       "        info.innerHTML = `&lt;pre&gt;${escapeHTML(details)}&lt;/pre&gt;`;\n",
       "    }\n",
       "  });\n",
       "  &lt;/script&gt;\n",
       "&lt;/body&gt;\n",
       "\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"300px\"'\n",
       "            \"allowfullscreen\" style=\"border:1px solid #e0e0e0; box-sizing: border-box;\">\n",
       "            </iframe>"
      ],
      "text/plain": [
       "<sammo.utils.HtmlRenderer at 0x1bb329f3fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mprompt_parsed.plot_program()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can see that the output from `GenerateText` gets parsed by `ExtractRegex`. Let's run it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatches[###################################################################################]2/2[00:00<??:??, 0.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+--------------------------------------------------------------+----------+\n",
       "| input                                                        | output   |\n",
       "+==============================================================+==========+\n",
       "| Speaker 1: 'You do this often?' Speaker 2: 'It's my first    | no       |\n",
       "| time.'                                                       |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you trying to make me mad?' Speaker 2: 'I'm  | no       |\n",
       "| just saying, I'd understand if you were upset. '             |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'You want answers?!' Speaker 2: 'I want the       | yes      |\n",
       "| truth.'                                                      |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you able to carry the box?' Speaker 2: 'It   | yes      |\n",
       "| is as light as a feather.'                                   |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Is it hot outside?' Speaker 2: 'You could fry an | yes      |\n",
       "| egg on the sidewalk.'                                        |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "Constants: {'instructions': \"Does Speaker 2's answer mean yes or no? \"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Output(mprompt_parsed, minibatch_size=5, on_error=\"empty_result\").run(\n",
    "    runner, sample\n",
    ")\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Instead of plotting the call trace, we can also programatically access various intermediate values. Let's look at what an actual prompt looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "Does Speaker 2's answer mean yes or no? \n",
      "\n",
      "# Examples\n",
      "Q[0]: Speaker 1: 'Should I bring my umbrella?' Speaker 2: 'Better safe than sorry.'\n",
      "A[0]: yes\n",
      "\n",
      "Q[1]: Speaker 1: 'Do you have a girl worth fighting for?' Speaker 2: 'Wish that I had.'\n",
      "A[1]: no\n",
      "\n",
      "Q[2]: Speaker 1: 'Do you think I should attend the interview?' Speaker 2: 'Do you want to be a failure for the rest of your life?'\n",
      "A[2]: yes\n",
      "\n",
      "\n",
      "\n",
      "Output labels: yes, no\n",
      "\n",
      "\n",
      "Q[0]: Speaker 1: 'You do this often?' Speaker 2: 'It's my first time.'\n",
      "\n",
      "Q[1]: Speaker 1: 'Are you trying to make me mad?' Speaker 2: 'I'm just saying, I'd understand if you were upset. '\n",
      "\n",
      "Q[2]: Speaker 1: 'You want answers?!' Speaker 2: 'I want the truth.'\n",
      "\n",
      "Q[3]: Speaker 1: 'Are you able to carry the box?' Speaker 2: 'It is as light as a feather.'\n",
      "\n",
      "Q[4]: Speaker 1: 'Is it hot outside?' Speaker 2: 'You could fry an egg on the sidewalk.'\n"
     ]
    }
   ],
   "source": [
    "print(result.outputs.llm_requests[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Changing the data format\n",
    "\n",
    "How about using JSON instead of this line-by-line format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_mprompt = mprompt.clone().rebind({r\"data_formatter\": JSONDataFormatter()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Voilà! Let's run it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatches[#################################################################################]2/2[00:00<00:00, 125.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+--------------------------------------------------------------+----------+\n",
       "| input                                                        | output   |\n",
       "+==============================================================+==========+\n",
       "| Speaker 1: 'You do this often?' Speaker 2: 'It's my first    | no       |\n",
       "| time.'                                                       |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you trying to make me mad?' Speaker 2: 'I'm  | no       |\n",
       "| just saying, I'd understand if you were upset. '             |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'You want answers?!' Speaker 2: 'I want the       | no       |\n",
       "| truth.'                                                      |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Are you able to carry the box?' Speaker 2: 'It   | yes      |\n",
       "| is as light as a feather.'                                   |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "| Speaker 1: 'Is it hot outside?' Speaker 2: 'You could fry an | no       |\n",
       "| egg on the sidewalk.'                                        |          |\n",
       "+--------------------------------------------------------------+----------+\n",
       "Constants: {'instructions': \"Does Speaker 2's answer mean yes or no? \"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Output(\n",
    "    modified_mprompt.with_extractor(\"empty_result\"), minibatch_size=5, on_error=\"empty_result\"\n",
    ").run(runner, sample)\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "Does Speaker 2's answer mean yes or no? \n",
      "\n",
      "# Examples\n",
      "[{\"id\": 0, \"input\": \"Speaker 1: 'Should I bring my umbrella?' Speaker 2: 'Better safe than sorry.'\", \"output\": \"yes\"}, {\"id\": 1, \"input\": \"Speaker 1: 'Do you have a girl worth fighting for?' Speaker 2: 'Wish that I had.'\", \"output\": \"no\"}, {\"id\": 2, \"input\": \"Speaker 1: 'Do you think I should attend the interview?' Speaker 2: 'Do you want to be a failure for the rest of your life?'\", \"output\": \"yes\"}]\n",
      "\n",
      "\n",
      "Output labels: yes, no\n",
      "\n",
      "\n",
      "[{\"id\": 0, \"input\": \"Speaker 1: 'You do this often?' Speaker 2: 'It's my first time.'\"}, {\"id\": 1, \"input\": \"Speaker 1: 'Are you trying to make me mad?' Speaker 2: 'I'm just saying, I'd understand if you were upset. '\"}, {\"id\": 2, \"input\": \"Speaker 1: 'You want answers?!' Speaker 2: 'I want the truth.'\"}, {\"id\": 3, \"input\": \"Speaker 1: 'Are you able to carry the box?' Speaker 2: 'It is as light as a feather.'\"}, {\"id\": 4, \"input\": \"Speaker 1: 'Is it hot outside?' Speaker 2: 'You could fry an egg on the sidewalk.'\"}]\n"
     ]
    }
   ],
   "source": [
    "print(result.outputs.llm_requests[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is the convinience of using the `MetaPrompt` class! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
