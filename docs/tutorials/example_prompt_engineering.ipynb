{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load from parent directory if not installed\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "if not importlib.util.find_spec(\"sammo\"):\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../../\")\n",
    "os.environ[\"CACHE_FILE\"] = \"cache/basic_prompt_engineering.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Basic Prompt Engineering\n",
    "\n",
    "`SAMMO` has a variety of tools that make trying out different sets of prompts easy. Let's start by loading the same task as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 3:25 _init.py\n",
    "import pathlib\n",
    "import sammo\n",
    "from sammo.runners import OpenAIChat\n",
    "from sammo.base import Template, EvaluationScore\n",
    "from sammo.components import Output, GenerateText, ForEach, Union\n",
    "from sammo.extractors import ExtractRegex\n",
    "from sammo.data import DataTable\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError(\"Please set the environment variable 'OPENAI_API_KEY'.\")\n",
    "\n",
    "_ = sammo.setup_logger(\"WARNING\")  # we're only interested in warnings for now\n",
    "\n",
    "runner = OpenAIChat(\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    api_config={\"api_key\": os.environ['OPENAI_API_KEY']},\n",
    "    cache=os.getenv(\"CACHE_FILE\", \"cache.tsv\"),\n",
    "    timeout=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    url=\"https://github.com/google/BIG-bench/raw/main/bigbench/benchmark_tasks/implicatures/task.json\",\n",
    "):\n",
    "    task = json.loads(requests.get(url).content)\n",
    "    # convert label to single string\n",
    "    for x in task[\"examples\"]:\n",
    "        x[\"output\"] = max(x[\"target_scores\"], key=x[\"target_scores\"].get)\n",
    "\n",
    "    return DataTable.from_records(\n",
    "        task[\"examples\"],\n",
    "        input_fields=\"input\",\n",
    "        constants={\"instructions\": task[\"task_prefix\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "mydata = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's say we want to try out different instructions. For that, let's define an objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load -s accuracy _init.py\n",
    "def accuracy(y_true: DataTable, y_pred: DataTable) -> EvaluationScore:\n",
    "    y_true = y_true.outputs.values\n",
    "    y_pred = y_pred.outputs.normalized_values()\n",
    "    n_correct = sum([y_p == y_t for y_p, y_t in zip(y_pred, y_true)])\n",
    "\n",
    "    return EvaluationScore(n_correct / len(y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Nothing special here - we simply count the number of correct labels and return an `EvaluationScore` object.\n",
    "\n",
    "To try out different prompts, we need to describe the space of possible candidates. `SAMMO` does that by offering a number of operators, such as `one_of`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sammo.search import EnumerativeSearch\n",
    "from sammo.search_op import one_of\n",
    "from sammo.base import Template\n",
    "from sammo.components import Output, GenerateText\n",
    "\n",
    "\n",
    "def labeling_prompt_space():\n",
    "    instructions = one_of(\n",
    "        [\n",
    "            \"Does the reply mean yes or no?\",\n",
    "            \"Does Speaker 2's answer mean yes or no? \"\n",
    "        ],\n",
    "        reference_id=\"instr\",\n",
    "    )\n",
    "    prompt = GenerateText(\n",
    "        Template(\n",
    "            \"Instructions:{{{instructions}}}\\nOutput labels: yes, no\\nInput: {{{input}}}\\nOutput:\",\n",
    "            instructions=instructions,\n",
    "        )\n",
    "    )\n",
    "    return Output(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "With the search space defined, we can now kick off the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate[###################################]2/2[00:00<00:00] >> minibatches (total)[#######################]50/50[00:00<00:00]\n",
      "\n",
      "Fitting log (2 entries):\n",
      "iteration    action                                            objective    costs                          parse_errors\n",
      "-----------  ------------------------------------------------  -----------  -----------------------------  --------------\n",
      "0            {'instr': \"'Does the reply mean yes or no?'\"}     0.68         {'input': 1348, 'output': 25}  0.0\n",
      "1            {'instr': '\"Does Speaker 2\\'s answer mean yes or  0.8          {'input': 1448, 'output': 25}  0.0\n",
      "             no? \"'}\n"
     ]
    }
   ],
   "source": [
    "sample = mydata.sample(25, seed=42)\n",
    "searcher = EnumerativeSearch(runner, labeling_prompt_space, accuracy)\n",
    "y_pred = searcher.fit_transform(sample)\n",
    "searcher.show_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Okay, we are doing a bit better! Let's see if changing the temperature would impact the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate[###################################]4/4[00:00<00:00] >> minibatches (total)[#####################]100/100[00:00<00:00]\n",
      "\n",
      "Fitting log (4 entries):\n",
      "iteration    action                                             objective    costs                          parse_errors\n",
      "-----------  -------------------------------------------------  -----------  -----------------------------  --------------\n",
      "0            {'instr': \"'Does Speakers 2 answer mean yes or no  0.64         {'input': 1498, 'output': 25}  0.0\n",
      "             to Speaker 1?'\", 'randomness': 0.7}\n",
      "1            {'instr': \"'Does Speakers 2 answer mean yes or no  0.76         {'input': 1498, 'output': 25}  0.0\n",
      "             to Speaker 1?'\", 'randomness': 1.0}\n",
      "2            {'instr': '\"Does Speaker 2\\'s answer mean yes or   0.8          {'input': 1448, 'output': 25}  0.0\n",
      "             no? \"', 'randomness': 0.7}\n",
      "3            {'instr': '\"Does Speaker 2\\'s answer mean yes or   0.64         {'input': 1448, 'output': 25}  0.0\n",
      "             no? \"', 'randomness': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def labeling_prompt_space():\n",
    "    instructions = one_of(\n",
    "        [\n",
    "            \"Does Speakers 2 answer mean yes or no to Speaker 1?\",\n",
    "            \"Does Speaker 2's answer mean yes or no? \"\n",
    "        ],\n",
    "        reference_id=\"instr\",\n",
    "    )\n",
    "    prompt = GenerateText(\n",
    "        Template(\n",
    "            \"Instructions:{{{instructions}}}\\nOutput labels: yes, no\\nInput: {{{input}}}\\nOutput:\",\n",
    "            instructions=instructions,\n",
    "        ),\n",
    "        randomness=one_of([0.7, 1.0], reference_id=\"randomness\"),\n",
    "    )\n",
    "    return Output(prompt)\n",
    "\n",
    "\n",
    "searcher = EnumerativeSearch(runner, labeling_prompt_space, accuracy)\n",
    "searcher.fit(sample)\n",
    "searcher.show_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Not bad! With `SAMMO`, we can quickly try out several alternatives if we want to manually tinker with different prompts.\n",
    "\n",
    "However, `SAMMO` offers a much more powerful way of automatically optimizing prompts which we cover in the section after the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
